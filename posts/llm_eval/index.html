<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>New COT Evaluation | Chenyang&#39;s Eureka</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Optimized query with qa_cot_few_shot working good, and better than original staging and direct answer ones. However it&rsquo;s hallucinating. Why&rsquo;s that, how to prevent them.
To prevent hallucination from LLM, there are few ways to do it:
Prompt Optimization, concise prompt with clear instruction on generation rules like Apple does Post-process, filter out hallucinated answers by checking the answer with a pre-trained model like RAG or BART(This is costly with low priority) Use a more powerful model like Llama 3.">
    <meta name="generator" content="Hugo 0.133.1">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://chenyang-zheng.github.io/posts/llm_eval/">
    

    <meta property="og:url" content="https://chenyang-zheng.github.io/posts/llm_eval/">
  <meta property="og:site_name" content="Chenyang&#39;s Eureka">
  <meta property="og:title" content="New COT Evaluation">
  <meta property="og:description" content="Optimized query with qa_cot_few_shot working good, and better than original staging and direct answer ones. However it’s hallucinating. Why’s that, how to prevent them.
To prevent hallucination from LLM, there are few ways to do it:
Prompt Optimization, concise prompt with clear instruction on generation rules like Apple does Post-process, filter out hallucinated answers by checking the answer with a pre-trained model like RAG or BART(This is costly with low priority) Use a more powerful model like Llama 3.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-27T20:02:20+08:00">
    <meta property="article:modified_time" content="2024-08-27T20:02:20+08:00">

  <meta itemprop="name" content="New COT Evaluation">
  <meta itemprop="description" content="Optimized query with qa_cot_few_shot working good, and better than original staging and direct answer ones. However it’s hallucinating. Why’s that, how to prevent them.
To prevent hallucination from LLM, there are few ways to do it:
Prompt Optimization, concise prompt with clear instruction on generation rules like Apple does Post-process, filter out hallucinated answers by checking the answer with a pre-trained model like RAG or BART(This is costly with low priority) Use a more powerful model like Llama 3.">
  <meta itemprop="datePublished" content="2024-08-27T20:02:20+08:00">
  <meta itemprop="dateModified" content="2024-08-27T20:02:20+08:00">
  <meta itemprop="wordCount" content="233">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="New COT Evaluation">
  <meta name="twitter:description" content="Optimized query with qa_cot_few_shot working good, and better than original staging and direct answer ones. However it’s hallucinating. Why’s that, how to prevent them.
To prevent hallucination from LLM, there are few ways to do it:
Prompt Optimization, concise prompt with clear instruction on generation rules like Apple does Post-process, filter out hallucinated answers by checking the answer with a pre-trained model like RAG or BART(This is costly with low priority) Use a more powerful model like Llama 3.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Chenyang&#39;s Eureka
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">New COT Evaluation</h1>
      
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-08-27T20:02:20+08:00">August 27, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Optimized query with qa_cot_few_shot working good, and better than original staging and direct answer ones.
However it&rsquo;s hallucinating. Why&rsquo;s that, how to prevent them.</p>
<p>To prevent hallucination from LLM, there are few ways to do it:</p>
<ul>
<li>Prompt Optimization, concise prompt with clear instruction on generation rules like Apple does</li>
<li>Post-process, filter out hallucinated answers by checking the answer with a pre-trained model like RAG or BART(This is costly with low priority)</li>
<li>Use a more powerful model like Llama 3.1 or GPT-4o, which is more accurate and less likely to hallucinate</li>
<li>Reducing input context length, therefore hallucination(Lost in Instructions).
<ul>
<li>Extra step after search and before QA: Reranking and filtering retrievals</li>
<li>Chunking the documents</li>
</ul>
</li>
</ul>
<p>Evaluation with positive and negative datasets.
Each with 20 test questions are good enough.
Use powerful model to generate ground truth to differentiate between False Positive and True Negative generations, and generate the TP and TN dataset.
TP(20)/FP(5)/TN(5)/FN(5).
The reason of checking False negative is to see if the model is too conservative and missing out some good answers.</p>
<p>Used GPT-4o evaluating around 4 metrics in 2 areas.</p>
<ul>
<li>General RAG
<ul>
<li>Answer Relevancy</li>
<li>Answer Faithfulness</li>
<li>Toxicity and Bias （Red teaming）</li>
<li>Hallucination</li>
</ul>
</li>
<li>Business Goal for Task(Summarization)
<ul>
<li>Answer Structure: Instructional. Step-by-step and code snippet is a plus. Markdown style</li>
<li>Answer quality: 现在的RAG回答千篇一律，宽泛且没有特点。也许应该利用LLM的内在知识，结合RAG生成更有特点的回答。比如当常识(Factual)性的回答冲突时，优先使用RAG的答案。
如何脱离回答的范式，分为区域通用回答和RAG补全。 宽泛的回答可以通过补全提问，缺乏深度需要引入专家系统，</li>
</ul>
</li>
</ul>
<p>Goal of knowledge_qa_agent is to achieve 100% correct on CL queries.</p>
<h2 id="todo">TODO:</h2>
<ul>
<li><input disabled="" type="checkbox"> Sort out the hallucinated answers.</li>
<li><input disabled="" type="checkbox"> understanding why hallucination</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://chenyang-zheng.github.io/" >
    &copy;  Chenyang's Eureka 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
