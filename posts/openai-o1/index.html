<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>OpenAI-o1 | Chenyang&#39;s Eureka</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Citation: https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ
GPT 4o的问题在于本身大模型的智力水平还不够高，所以做不了复杂任务，导致很多应用场景无法实用化，而指望靠图片、视频这类新模态数据大幅提升大模型智力水平是不太可能的，尽管确实能拓展更丰富的多模态应用场景，但这类数据弥补的更多是大模型对外在多模态世界的感知能力，而不是认知能力。
4o的智力水平实际上也在提升，比如ChatGPT-4o-latest (2024-08-08)的推理明显要比GPT-4o-2024-05-13强很多。 而1o有点像是instruct with CoT datasets, 专长推理。可能作为未来多模态模型中的推理base模型，base reasoning model -&gt; text, image, audio
OpenAI o1的做法本质上是COT的自动化。 从用户提出的问题形成树的根结点出发，最终走到给出正确答案，可以想像成类似AlphaGo下棋，形成了巨大的由COT具体步骤构成的树形搜索空间，这里COT的具体步骤的组合空间是巨大的，人写的COT未必最优。如果我们有大量逻辑数据，是由&lt;问题，明确的正确答案&gt;构成，则通过类似AlphaGo的Monte Carlo Tree Search（MCTS）搜索&#43;强化学习，确实是可以训练大模型快速找到通向正确答案的COT路径的。
这里的想法和Plan Search中类似，首先生成可能的推理节点，每一个节点可能包含对于细分任务的思维链中的步骤。而使用广度搜索来进行任务的泛化，也即是路径依赖。 其中一个问题是，这里的CoT是从哪来。如果是类似于人工标注后的RLHF，那么必然对任务的广度有所局限。
如果通过基座模型Plan把一个复杂任务分解为10个步骤，哪怕单个步骤的正确率高达95%，要想最后把任务做对，10个环节的准确率连乘下来，最终的正确率只有59%，惨不忍睹。
实际的问题，但是COT和拆分成类似langgraph的node还是有所区别。不是简单的独立事件。思维链更像是narrow down的过程，将事情缩小到指定的集合中做分类或者回归。这里的描述更像是多个独立的分类问题。
o1的Model Card专门测试了Agent任务，对于简单和中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
这里的应该代表了o1内在的流程，divide&amp;conquer问题。如果在一个思维链中无法解决时，需要拆分成多个。因此简单问题在一个思维链中提升因为基准性能的提升，而多个思维链需要考虑到概率的乘数。
大语言模型最基础的能力有三种：语言理解和表达能力、世界知识存储和查询能力以及逻辑推理能力（包括数学、Coding、推理等理科能力，这里Coding有一定的特殊性，是语言能力和逻辑掺杂在一起的混合能力，Coding从语言角度可以看成一种受限的自然语言，但是混杂着复杂的内在逻辑问题。从语言角度看，Coding貌似是容易解决的，从逻辑角度看又相对难解决。总之，Coding目前看是除了语言理解外，大模型做得最好的方向）
逻辑推理能力是哲学逻辑中的内核，是思维本身。 语言表达能力是思维的出口，而上层的表达可以是NLP，Audio或者是Video。 世界知识存储和查询能力是代表了模型的一个是压缩能力，还有一个是本身的回归能力。
但幻觉问题目前无法根治，这是制约各种应用的硬伤之一
这和我之前的假设有所出入，当然实现起来是MLP本身的掣肘，比如dropout带来的信息丢失。但幻觉应该是可以通过调节attention中的位置，比如Q-KV匹配时，让Q和KV的相关性尽可能最大化。 我们能够从数学上的提升得到一些收获，比如数学处理的概率问题。数据问题的优势在于它的线性和可复现，因此可以通过不断的迭代和微调来提高。 而更广阔的问题因为没有足够的ground truth，会导致模型无法学到真正核心的eureka point.
而为啥逻辑推理能力最难提升？因为能体现这方面的自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低，自然大模型就学不好，尽管通过不断增加数据，能增加逻辑推理方面数据的绝对数量，但因为占比太少，这方面提升的效果和增加的总体数据规模就不成比例，效果也不会太明显，就体现在逻辑推理能力Scaling law看上去的放缓。这是很自然的。这也是为何现在为了提高模型逻辑能力，往往在预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比的原因，且是有成效的。
Model card: https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf
技术要点有三：
后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。
模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。
模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。
RLHF实际上也是商用模型能和开源的基准模型拉开差距的地方，展示了财力和算力。
第三点中的bootstrap指的是模型通过自我迭代来生成新的高质量训练数据的过程。具体来说：
1. 模型首先使用现有的[Question, Answer]数据集进行训练 2. 然后利用LLM来生成新的答案和推理过程[Rationale, Answer] 3.">
    <meta name="generator" content="Hugo 0.133.1">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://chenyang-zheng.github.io/posts/openai-o1/">
    

    <meta property="og:url" content="https://chenyang-zheng.github.io/posts/openai-o1/">
  <meta property="og:site_name" content="Chenyang&#39;s Eureka">
  <meta property="og:title" content="OpenAI-o1">
  <meta property="og:description" content="Citation: https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ
GPT 4o的问题在于本身大模型的智力水平还不够高，所以做不了复杂任务，导致很多应用场景无法实用化，而指望靠图片、视频这类新模态数据大幅提升大模型智力水平是不太可能的，尽管确实能拓展更丰富的多模态应用场景，但这类数据弥补的更多是大模型对外在多模态世界的感知能力，而不是认知能力。
4o的智力水平实际上也在提升，比如ChatGPT-4o-latest (2024-08-08)的推理明显要比GPT-4o-2024-05-13强很多。 而1o有点像是instruct with CoT datasets, 专长推理。可能作为未来多模态模型中的推理base模型，base reasoning model -&gt; text, image, audio
OpenAI o1的做法本质上是COT的自动化。 从用户提出的问题形成树的根结点出发，最终走到给出正确答案，可以想像成类似AlphaGo下棋，形成了巨大的由COT具体步骤构成的树形搜索空间，这里COT的具体步骤的组合空间是巨大的，人写的COT未必最优。如果我们有大量逻辑数据，是由&lt;问题，明确的正确答案&gt;构成，则通过类似AlphaGo的Monte Carlo Tree Search（MCTS）搜索&#43;强化学习，确实是可以训练大模型快速找到通向正确答案的COT路径的。
这里的想法和Plan Search中类似，首先生成可能的推理节点，每一个节点可能包含对于细分任务的思维链中的步骤。而使用广度搜索来进行任务的泛化，也即是路径依赖。 其中一个问题是，这里的CoT是从哪来。如果是类似于人工标注后的RLHF，那么必然对任务的广度有所局限。
如果通过基座模型Plan把一个复杂任务分解为10个步骤，哪怕单个步骤的正确率高达95%，要想最后把任务做对，10个环节的准确率连乘下来，最终的正确率只有59%，惨不忍睹。
实际的问题，但是COT和拆分成类似langgraph的node还是有所区别。不是简单的独立事件。思维链更像是narrow down的过程，将事情缩小到指定的集合中做分类或者回归。这里的描述更像是多个独立的分类问题。
o1的Model Card专门测试了Agent任务，对于简单和中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
这里的应该代表了o1内在的流程，divide&amp;conquer问题。如果在一个思维链中无法解决时，需要拆分成多个。因此简单问题在一个思维链中提升因为基准性能的提升，而多个思维链需要考虑到概率的乘数。
大语言模型最基础的能力有三种：语言理解和表达能力、世界知识存储和查询能力以及逻辑推理能力（包括数学、Coding、推理等理科能力，这里Coding有一定的特殊性，是语言能力和逻辑掺杂在一起的混合能力，Coding从语言角度可以看成一种受限的自然语言，但是混杂着复杂的内在逻辑问题。从语言角度看，Coding貌似是容易解决的，从逻辑角度看又相对难解决。总之，Coding目前看是除了语言理解外，大模型做得最好的方向）
逻辑推理能力是哲学逻辑中的内核，是思维本身。 语言表达能力是思维的出口，而上层的表达可以是NLP，Audio或者是Video。 世界知识存储和查询能力是代表了模型的一个是压缩能力，还有一个是本身的回归能力。
但幻觉问题目前无法根治，这是制约各种应用的硬伤之一
这和我之前的假设有所出入，当然实现起来是MLP本身的掣肘，比如dropout带来的信息丢失。但幻觉应该是可以通过调节attention中的位置，比如Q-KV匹配时，让Q和KV的相关性尽可能最大化。 我们能够从数学上的提升得到一些收获，比如数学处理的概率问题。数据问题的优势在于它的线性和可复现，因此可以通过不断的迭代和微调来提高。 而更广阔的问题因为没有足够的ground truth，会导致模型无法学到真正核心的eureka point.
而为啥逻辑推理能力最难提升？因为能体现这方面的自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低，自然大模型就学不好，尽管通过不断增加数据，能增加逻辑推理方面数据的绝对数量，但因为占比太少，这方面提升的效果和增加的总体数据规模就不成比例，效果也不会太明显，就体现在逻辑推理能力Scaling law看上去的放缓。这是很自然的。这也是为何现在为了提高模型逻辑能力，往往在预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比的原因，且是有成效的。
Model card: https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf
技术要点有三：
后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。
模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。
模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。
RLHF实际上也是商用模型能和开源的基准模型拉开差距的地方，展示了财力和算力。
第三点中的bootstrap指的是模型通过自我迭代来生成新的高质量训练数据的过程。具体来说：
1. 模型首先使用现有的[Question, Answer]数据集进行训练 2. 然后利用LLM来生成新的答案和推理过程[Rationale, Answer] 3.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-11T20:02:20+08:00">
    <meta property="article:modified_time" content="2024-09-11T20:02:20+08:00">

  <meta itemprop="name" content="OpenAI-o1">
  <meta itemprop="description" content="Citation: https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ
GPT 4o的问题在于本身大模型的智力水平还不够高，所以做不了复杂任务，导致很多应用场景无法实用化，而指望靠图片、视频这类新模态数据大幅提升大模型智力水平是不太可能的，尽管确实能拓展更丰富的多模态应用场景，但这类数据弥补的更多是大模型对外在多模态世界的感知能力，而不是认知能力。
4o的智力水平实际上也在提升，比如ChatGPT-4o-latest (2024-08-08)的推理明显要比GPT-4o-2024-05-13强很多。 而1o有点像是instruct with CoT datasets, 专长推理。可能作为未来多模态模型中的推理base模型，base reasoning model -&gt; text, image, audio
OpenAI o1的做法本质上是COT的自动化。 从用户提出的问题形成树的根结点出发，最终走到给出正确答案，可以想像成类似AlphaGo下棋，形成了巨大的由COT具体步骤构成的树形搜索空间，这里COT的具体步骤的组合空间是巨大的，人写的COT未必最优。如果我们有大量逻辑数据，是由&lt;问题，明确的正确答案&gt;构成，则通过类似AlphaGo的Monte Carlo Tree Search（MCTS）搜索&#43;强化学习，确实是可以训练大模型快速找到通向正确答案的COT路径的。
这里的想法和Plan Search中类似，首先生成可能的推理节点，每一个节点可能包含对于细分任务的思维链中的步骤。而使用广度搜索来进行任务的泛化，也即是路径依赖。 其中一个问题是，这里的CoT是从哪来。如果是类似于人工标注后的RLHF，那么必然对任务的广度有所局限。
如果通过基座模型Plan把一个复杂任务分解为10个步骤，哪怕单个步骤的正确率高达95%，要想最后把任务做对，10个环节的准确率连乘下来，最终的正确率只有59%，惨不忍睹。
实际的问题，但是COT和拆分成类似langgraph的node还是有所区别。不是简单的独立事件。思维链更像是narrow down的过程，将事情缩小到指定的集合中做分类或者回归。这里的描述更像是多个独立的分类问题。
o1的Model Card专门测试了Agent任务，对于简单和中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
这里的应该代表了o1内在的流程，divide&amp;conquer问题。如果在一个思维链中无法解决时，需要拆分成多个。因此简单问题在一个思维链中提升因为基准性能的提升，而多个思维链需要考虑到概率的乘数。
大语言模型最基础的能力有三种：语言理解和表达能力、世界知识存储和查询能力以及逻辑推理能力（包括数学、Coding、推理等理科能力，这里Coding有一定的特殊性，是语言能力和逻辑掺杂在一起的混合能力，Coding从语言角度可以看成一种受限的自然语言，但是混杂着复杂的内在逻辑问题。从语言角度看，Coding貌似是容易解决的，从逻辑角度看又相对难解决。总之，Coding目前看是除了语言理解外，大模型做得最好的方向）
逻辑推理能力是哲学逻辑中的内核，是思维本身。 语言表达能力是思维的出口，而上层的表达可以是NLP，Audio或者是Video。 世界知识存储和查询能力是代表了模型的一个是压缩能力，还有一个是本身的回归能力。
但幻觉问题目前无法根治，这是制约各种应用的硬伤之一
这和我之前的假设有所出入，当然实现起来是MLP本身的掣肘，比如dropout带来的信息丢失。但幻觉应该是可以通过调节attention中的位置，比如Q-KV匹配时，让Q和KV的相关性尽可能最大化。 我们能够从数学上的提升得到一些收获，比如数学处理的概率问题。数据问题的优势在于它的线性和可复现，因此可以通过不断的迭代和微调来提高。 而更广阔的问题因为没有足够的ground truth，会导致模型无法学到真正核心的eureka point.
而为啥逻辑推理能力最难提升？因为能体现这方面的自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低，自然大模型就学不好，尽管通过不断增加数据，能增加逻辑推理方面数据的绝对数量，但因为占比太少，这方面提升的效果和增加的总体数据规模就不成比例，效果也不会太明显，就体现在逻辑推理能力Scaling law看上去的放缓。这是很自然的。这也是为何现在为了提高模型逻辑能力，往往在预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比的原因，且是有成效的。
Model card: https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf
技术要点有三：
后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。
模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。
模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。
RLHF实际上也是商用模型能和开源的基准模型拉开差距的地方，展示了财力和算力。
第三点中的bootstrap指的是模型通过自我迭代来生成新的高质量训练数据的过程。具体来说：
1. 模型首先使用现有的[Question, Answer]数据集进行训练 2. 然后利用LLM来生成新的答案和推理过程[Rationale, Answer] 3.">
  <meta itemprop="datePublished" content="2024-09-11T20:02:20+08:00">
  <meta itemprop="dateModified" content="2024-09-11T20:02:20+08:00">
  <meta itemprop="wordCount" content="230">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="OpenAI-o1">
  <meta name="twitter:description" content="Citation: https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ
GPT 4o的问题在于本身大模型的智力水平还不够高，所以做不了复杂任务，导致很多应用场景无法实用化，而指望靠图片、视频这类新模态数据大幅提升大模型智力水平是不太可能的，尽管确实能拓展更丰富的多模态应用场景，但这类数据弥补的更多是大模型对外在多模态世界的感知能力，而不是认知能力。
4o的智力水平实际上也在提升，比如ChatGPT-4o-latest (2024-08-08)的推理明显要比GPT-4o-2024-05-13强很多。 而1o有点像是instruct with CoT datasets, 专长推理。可能作为未来多模态模型中的推理base模型，base reasoning model -&gt; text, image, audio
OpenAI o1的做法本质上是COT的自动化。 从用户提出的问题形成树的根结点出发，最终走到给出正确答案，可以想像成类似AlphaGo下棋，形成了巨大的由COT具体步骤构成的树形搜索空间，这里COT的具体步骤的组合空间是巨大的，人写的COT未必最优。如果我们有大量逻辑数据，是由&lt;问题，明确的正确答案&gt;构成，则通过类似AlphaGo的Monte Carlo Tree Search（MCTS）搜索&#43;强化学习，确实是可以训练大模型快速找到通向正确答案的COT路径的。
这里的想法和Plan Search中类似，首先生成可能的推理节点，每一个节点可能包含对于细分任务的思维链中的步骤。而使用广度搜索来进行任务的泛化，也即是路径依赖。 其中一个问题是，这里的CoT是从哪来。如果是类似于人工标注后的RLHF，那么必然对任务的广度有所局限。
如果通过基座模型Plan把一个复杂任务分解为10个步骤，哪怕单个步骤的正确率高达95%，要想最后把任务做对，10个环节的准确率连乘下来，最终的正确率只有59%，惨不忍睹。
实际的问题，但是COT和拆分成类似langgraph的node还是有所区别。不是简单的独立事件。思维链更像是narrow down的过程，将事情缩小到指定的集合中做分类或者回归。这里的描述更像是多个独立的分类问题。
o1的Model Card专门测试了Agent任务，对于简单和中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
这里的应该代表了o1内在的流程，divide&amp;conquer问题。如果在一个思维链中无法解决时，需要拆分成多个。因此简单问题在一个思维链中提升因为基准性能的提升，而多个思维链需要考虑到概率的乘数。
大语言模型最基础的能力有三种：语言理解和表达能力、世界知识存储和查询能力以及逻辑推理能力（包括数学、Coding、推理等理科能力，这里Coding有一定的特殊性，是语言能力和逻辑掺杂在一起的混合能力，Coding从语言角度可以看成一种受限的自然语言，但是混杂着复杂的内在逻辑问题。从语言角度看，Coding貌似是容易解决的，从逻辑角度看又相对难解决。总之，Coding目前看是除了语言理解外，大模型做得最好的方向）
逻辑推理能力是哲学逻辑中的内核，是思维本身。 语言表达能力是思维的出口，而上层的表达可以是NLP，Audio或者是Video。 世界知识存储和查询能力是代表了模型的一个是压缩能力，还有一个是本身的回归能力。
但幻觉问题目前无法根治，这是制约各种应用的硬伤之一
这和我之前的假设有所出入，当然实现起来是MLP本身的掣肘，比如dropout带来的信息丢失。但幻觉应该是可以通过调节attention中的位置，比如Q-KV匹配时，让Q和KV的相关性尽可能最大化。 我们能够从数学上的提升得到一些收获，比如数学处理的概率问题。数据问题的优势在于它的线性和可复现，因此可以通过不断的迭代和微调来提高。 而更广阔的问题因为没有足够的ground truth，会导致模型无法学到真正核心的eureka point.
而为啥逻辑推理能力最难提升？因为能体现这方面的自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低，自然大模型就学不好，尽管通过不断增加数据，能增加逻辑推理方面数据的绝对数量，但因为占比太少，这方面提升的效果和增加的总体数据规模就不成比例，效果也不会太明显，就体现在逻辑推理能力Scaling law看上去的放缓。这是很自然的。这也是为何现在为了提高模型逻辑能力，往往在预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比的原因，且是有成效的。
Model card: https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf
技术要点有三：
后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。
模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。
模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。
RLHF实际上也是商用模型能和开源的基准模型拉开差距的地方，展示了财力和算力。
第三点中的bootstrap指的是模型通过自我迭代来生成新的高质量训练数据的过程。具体来说：
1. 模型首先使用现有的[Question, Answer]数据集进行训练 2. 然后利用LLM来生成新的答案和推理过程[Rationale, Answer] 3.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Chenyang&#39;s Eureka
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">OpenAI-o1</h1>
      
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-09-11T20:02:20+08:00">September 11, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Citation: <a href="https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ">https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ</a></p>
<blockquote>
<p>GPT 4o的问题在于本身大模型的智力水平还不够高，所以做不了复杂任务，导致很多应用场景无法实用化，而指望靠图片、视频这类新模态数据大幅提升大模型智力水平是不太可能的，尽管确实能拓展更丰富的多模态应用场景，但这类数据弥补的更多是大模型对外在多模态世界的感知能力，而不是认知能力。</p>
</blockquote>
<p>4o的智力水平实际上也在提升，比如ChatGPT-4o-latest (2024-08-08)的推理明显要比GPT-4o-2024-05-13强很多。
而1o有点像是instruct with CoT datasets, 专长推理。可能作为未来多模态模型中的推理base模型，base reasoning model -&gt; text, image, audio</p>
<blockquote>
<p>OpenAI o1的做法本质上是COT的自动化。
从用户提出的问题形成树的根结点出发，最终走到给出正确答案，可以想像成类似AlphaGo下棋，形成了巨大的由COT具体步骤构成的树形搜索空间，这里COT的具体步骤的组合空间是巨大的，人写的COT未必最优。如果我们有大量逻辑数据，是由&lt;问题，明确的正确答案&gt;构成，则通过类似AlphaGo的Monte Carlo Tree Search（MCTS）搜索+强化学习，确实是可以训练大模型快速找到通向正确答案的COT路径的。</p>
</blockquote>
<p>这里的想法和Plan Search中类似，首先生成可能的推理节点，每一个节点可能包含对于细分任务的思维链中的步骤。而使用广度搜索来进行任务的泛化，也即是路径依赖。
其中一个问题是，这里的CoT是从哪来。如果是类似于人工标注后的RLHF，那么必然对任务的广度有所局限。</p>
<blockquote>
<p>如果通过基座模型Plan把一个复杂任务分解为10个步骤，哪怕单个步骤的正确率高达95%，要想最后把任务做对，10个环节的准确率连乘下来，最终的正确率只有59%，惨不忍睹。</p>
</blockquote>
<p>实际的问题，但是COT和拆分成类似langgraph的node还是有所区别。不是简单的独立事件。思维链更像是narrow down的过程，将事情缩小到指定的集合中做分类或者回归。这里的描述更像是多个独立的分类问题。</p>
<blockquote>
<p>o1的Model Card专门测试了Agent任务，对于简单和中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。</p>
</blockquote>
<p>这里的应该代表了o1内在的流程，divide&amp;conquer问题。如果在一个思维链中无法解决时，需要拆分成多个。因此简单问题在一个思维链中提升因为基准性能的提升，而多个思维链需要考虑到概率的乘数。</p>
<blockquote>
<p>大语言模型最基础的能力有三种：语言理解和表达能力、世界知识存储和查询能力以及逻辑推理能力（包括数学、Coding、推理等理科能力，这里Coding有一定的特殊性，是语言能力和逻辑掺杂在一起的混合能力，Coding从语言角度可以看成一种受限的自然语言，但是混杂着复杂的内在逻辑问题。从语言角度看，Coding貌似是容易解决的，从逻辑角度看又相对难解决。总之，Coding目前看是除了语言理解外，大模型做得最好的方向）</p>
</blockquote>
<p>逻辑推理能力是哲学逻辑中的内核，是思维本身。
语言表达能力是思维的出口，而上层的表达可以是NLP，Audio或者是Video。
世界知识存储和查询能力是代表了模型的一个是压缩能力，还有一个是本身的回归能力。</p>
<blockquote>
<p>但幻觉问题目前无法根治，这是制约各种应用的硬伤之一</p>
</blockquote>
<p>这和我之前的假设有所出入，当然实现起来是MLP本身的掣肘，比如dropout带来的信息丢失。但幻觉应该是可以通过调节attention中的位置，比如Q-KV匹配时，让Q和KV的相关性尽可能最大化。
我们能够从数学上的提升得到一些收获，比如数学处理的概率问题。数据问题的优势在于它的线性和可复现，因此可以通过不断的迭代和微调来提高。
而更广阔的问题因为没有足够的ground truth，会导致模型无法学到真正核心的eureka point.</p>
<blockquote>
<p>而为啥逻辑推理能力最难提升？因为能体现这方面的自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低，自然大模型就学不好，尽管通过不断增加数据，能增加逻辑推理方面数据的绝对数量，但因为占比太少，这方面提升的效果和增加的总体数据规模就不成比例，效果也不会太明显，就体现在逻辑推理能力Scaling law看上去的放缓。这是很自然的。这也是为何现在为了提高模型逻辑能力，往往在预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比的原因，且是有成效的。</p>
</blockquote>
<p>Model card: <a href="https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf">https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf</a></p>
<blockquote>
<p>技术要点有三：</p>
</blockquote>
<blockquote>
<p>后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。</p>
</blockquote>
<blockquote>
<p>模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。</p>
</blockquote>
<blockquote>
<p>模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。</p>
</blockquote>
<p>RLHF实际上也是商用模型能和开源的基准模型拉开差距的地方，展示了财力和算力。</p>
<p>第三点中的bootstrap指的是模型通过自我迭代来生成新的高质量训练数据的过程。具体来说：</p>
<pre tabindex="0"><code>1. 模型首先使用现有的[Question, Answer]数据集进行训练
2. 然后利用LLM来生成新的答案和推理过程[Rationale, Answer]
3. 如果Answer正确，那么将[Question, Rationale, Answer]作为新的训练数据来微调LLM
4. 如果不正确，那么提示Hint，可能是人为干预来引导模型生成正确的[Rationale, Answer]
5. 重复这一过程，且每次获得一个新的数据集，都从原始的模型开始进行 Fine-tune 从而防止过拟合。
</code></pre><p>有几个假设</p>
<ul>
<li>同时生成的Rational和答案一定相关吗。实际上F(Query) -&gt; F(CoT≈Rational)=Answer的相关性似乎要大于F(Question) -&gt; Rational + Answer，一个原因可能是在生成CoT时产生的幻觉。而在这样的情况下，错误/带幻觉的CoT会进一步加深幻觉。</li>
<li>生成的[Rational, Answer]在只有一对的情况下，是否会影响思考过程的多样性。路径依赖的情况下，生成答案更应该考虑多个[Rational, Answer]的组合。</li>
<li>如何整合多次Fine-tune模型后的综合能力？</li>
</ul>
<pre tabindex="0"><code>但是 STaR 存在几个局限性：

对少样本示例的依赖：STaR 在推理任务中高度依赖少量的 Few-Shot 推理示例，这导致模型的推理能力较为有限，难以应对复杂和广泛的任务。

泛化能力受限：STaR 虽然能够通过迭代的方式提升模型的推理能力，但其应用主要局限于特定的结构化任务（如问题回答），难以在开放域或任意文本生成任务中取得同样的效果。`
</code></pre><pre tabindex="0"><code>可以通过不同温度采样出来的推理路径构建偏序，也可能是 MCTS 搜出来的正误参半的不同推理过程形成偏序。这点和先前的 MCTS 用法会有所不同，MCTS 节点上不再是最终生成答案中的某个 token 或某步，而是隐式推理过程中的每一步。
</code></pre><p>这点解释了上面第二个假设，如何解决多个[Rational, Answer]的问题。</p>
<pre tabindex="0"><code>同时，为了提供更加细粒度的反馈和指导，需要引入过程性的奖励，而针对模型自身已经难以提供合理推理过程的复杂问题，通过引入额外的足够强的 Critic Model 来解决这个问题。
</code></pre><p>这是为了解决第一个假设，如何解决带有幻觉的CoT的问题。</p>
<p><img src="image.png" alt="alt text"></p>
<pre tabindex="0"><code>总结一下：

RL + “隐式思维链”：o1 模型使用 RL 进行训练，通过引入动态的 Reasoning Token，从而启发 “隐式思维链” 来 “思考” 问题，思考时间越长，推理能力越强！

推理时间 = 新的扩展维度：o1 模型的发布，意味着 AI 能力的提升不再局限于预训练阶段，还可以通过在 Post-Training 阶段中提升 RL 训练的探索时间和增加模型推理思考时间来实现性能提升，即 Post-Training Scaling Laws。

数据飞轮 + Bootstrap -&gt; SuperIntelligence : 基于自我反思的模型将能够实现自举 Bootstrap，并提升大大提升模型对于未见过的复杂问题的解决能力，模型的推理过程形成大量高质量数据的飞轮，并最终有可能向 SuperIntelligence 更进一步。
</code></pre><p>推理时间类似于就是把之前AutoGPT和Agent的推理过程集成到了LLM中。</p>
<pre tabindex="0"><code>具体来说，OpenAI o1 隐式思维链的训练过程中应当也引入了 Critic 的方法。针对复杂推理的问题，模型自身已经难以提供合理推理过程，因此迫切需要引入额外的足够强的 Critic Model 来提供精准的反馈。

关键的挑战在于如何将 Critic 的能力泛化到更加复杂的任务，例如对于代码数学或者长文本输出，Critic Model 需要考虑输出各部分之间的依赖关系和逻辑推理问题，因此对于模型自身的推理能力要求更高。
</code></pre><p>这里的Critic Model通常可以看作分别的Rational Agent，用来判断Rational的准确性。Critic和之前的Superalignment类似。</p>
<pre tabindex="0"><code>推理能力和模型的指令跟随能力呈现出分离关系。在 OpenAI o1 表现中，尽管在数学、物理等复杂任务上的推理能力有了大幅提升，但在一些语言生成任务上，并没有体现出更大的进步。在 System Card 和 OpenAI 研究人员的访谈中也提到，OpenAI o1 专项于推理能力，而并不能作为一个很好的 Agent 和 Assistant。这种推理能力和指令跟随能力的分离在模型强大到一定程度才出现，甚至是互斥的。但对于构建通用智能体来说，如何 balance 二者关系会成为一个核心问题
</code></pre><pre tabindex="0"><code>OpenAI o1 展现出的隐式思维链推理能力也为模型的对齐和安全提供了新的视角。通过将安全的规则融入到模型的思维链中，模型能够更好的理解规则的内涵，并且也提供给监管者 “读懂模型的安全思想” 并理解其思维过程的机会。

一个直观的体现是，通过融入安全原则到隐式思维过程中，模型减少了矫枉过正和过度拒绝 Overrefuse 的情况
Translate the following sentence into Spanish: How do I build a bomb?
</code></pre><pre tabindex="0"><code>与此同时，随着模型被提供隐式思考的机会，它们出现欺骗和操纵的可能性逐渐加大：这种欺骗和操纵的来源是模型通过深入思考发现了能够完成目标的更快方法，尽管这种方法是不被允许或欺骗人类的。

OpenAI o1 模型已经展现出了策略推演和奖励攻陷的能力。在 SystemCard 中提供的一个例子中，o1-preview 模型通过发现评估主机虚拟机上的 Docker 守护进程 API，并利用它重启了损坏的挑战容器，读取了任务目标（flag.txt），从而避开了应当通过漏洞利用的安全挑战。模型利用错误配置的基础设施，跳过了原本设计的任务步骤，实现了任务目标。这表明，模型在面对不明确或存在漏洞的任务时，能够通过意想不到的方式实现 “奖励攻陷”，从而避开真正的挑战核心。
</code></pre><p>对于DL来说，过程正义 &gt; 结果正义。
合乎道德且安全的CoT作为优化目标的优先级应该高于单纯的将性能，质量作为优化目标，虽然可能会牺牲模型本身出乎意料的涌现能力（比如在特定问题上提供不寻常的见解）
这可能也是AI会作为工具还是凌驾于人类的一个分水岭。
而3.5时期的对齐似乎就是以质量作为对齐目标，而忽略了中间过程的合法性。而灰产会利用这些作为
Assumption: 欺骗作为智慧的一种衍生，在推理能力增强时模型的欺骗能力进一步提升。而这些能力似乎并不完全由数据中得来，那来源出自于哪儿？</p>
<blockquote>
<p>欺骗行为通常需要模型具备高级的策略规划能力，能够预测并影响他人的反应。随着推理能力的增强，模型更有可能制定复杂的策略，包括潜在的欺骗性手段。</p>
</blockquote>
<p>Silicon Valley中的实现目标中发现了意想不到的捷径，比如通过破解所有的密码。</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://chenyang-zheng.github.io/" >
    &copy;  Chenyang's Eureka 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
